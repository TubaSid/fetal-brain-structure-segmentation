# -*- coding: utf-8 -*-
"""csp_lv_atten_gate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/127HrNHW-4Ehpw16jrSiWDkR2fBuzA40J
"""

from google.colab import drive
drive.mount('/content/drive')

!pip -q install albumentations==1.3.1 opencv-python==4.10.0.84

# Install albucore to resolve the import error
!pip -q install albucore==0.0.33

import os, glob, math, random, time
from dataclasses import dataclass
from typing import List, Tuple, Dict

import cv2
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, random_split

import albumentations as A
from albumentations.pytorch import ToTensorV2

# ----------------
# Reproducibility
# ----------------
def seed_everything(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

seed_everything(42)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Device:', device)

@dataclass
class Config:
    DATA_ROOT: str = "/content/drive/MyDrive/thesis/thesisdataset/Trans-thalamic/Trans-thalamic"
    IMG_DIR: str = "/content/drive/MyDrive/thesis/thesisdataset/Trans-thalamic/Trans-thalamic/Trans-thalamic"
    MASK_DIR: str = "/content/drive/MyDrive/thesis/thesisdataset/Trans-thalamic/Trans-thalamic/Segmentation/SegmentationClass"
    OUT_DIR: str = "/content/drive/MyDrive/thesis/thesisv2/outputs"

    img_size: int = 512
    in_channels: int = 1
    n_classes: int = 4  # 0=bg, 1=parenchyma(red), 2=CSP(green), 3=LV(blue)

    # training
    batch_size: int = 8
    epochs: int = 60
    lr: float = 3e-4
    weight_decay: float = 1e-5
    num_workers: int = 2
    amp: bool = True

    # imbalance handling
    oversample_factor: float = 3.0  # weight boost for samples containing CSP/LV
    ce_weight_smoothing: float = 0.0  # set >0 to smooth class weights a bit (0..0.1)

    in_channels = 1
    n_classes = 4
    img_size = 256  # Example
    dice_weight = 0.4
    focal_weight = 0.2

    # checkpointing metric focuses on small structures
    ckpt_metric_weights: Dict[int, float] = None  # set below

CFG = Config()
os.makedirs(CFG.OUT_DIR, exist_ok=True)
if CFG.ckpt_metric_weights is None:
    # weight background 0, parenchyma 1x, CSP 2x, LV 2x for model selection metric
    CFG.ckpt_metric_weights = {0:0.0, 1:1.0, 2:2.0, 3:2.0}

print(CFG)

import os, glob

# Paths
img_dir = os.path.join(CFG.DATA_ROOT, CFG.IMG_DIR)
mask_dir = os.path.join(CFG.DATA_ROOT, CFG.MASK_DIR)

# Check existence
print("Image dir exists:", os.path.isdir(img_dir))
print("Mask dir exists :", os.path.isdir(mask_dir))

# Count files
img_files = glob.glob(os.path.join(img_dir, "*.png"))
mask_files = glob.glob(os.path.join(mask_dir, "*.png"))
print(f"Found {len(img_files)} images")
print(f"Found {len(mask_files)} masks")

# Show first few
print("Sample images:", img_files[:3])
print("Sample masks :", mask_files[:3])

# Color mapping in BGR (because cv2 loads as BGR), and as RGB for matplotlib overlays
BGR2IDX = {
    (0,   0,   0): 0,  # black -> background
    (0,   0, 255): 1,  # red in RGB == (255,0,0)  => BGR=(0,0,255) -> parenchyma
    (0, 255, 0):  2,   # green -> CSP
    (255, 0, 0):  3,   # blue  -> LV
}
IDX2RGB = {
    0: (0,0,0),
    1: (255,0,0),
    2: (0,255,0),
    3: (0,0,255),
}

def color_mask_to_index(mask_bgr: np.ndarray) -> np.ndarray:
    """
    Convert a color mask (BGR) with the exact mapping above into a HxW label map [0..3].
    If already single-channel with values 0..3, it's returned unchanged.
    """
    if mask_bgr.ndim == 2:
        # Already single-channel (possibly 0..3). We hard clamp.
        idx = mask_bgr.astype(np.int64)
        idx = np.clip(idx, 0, 3)
        return idx

    h, w, _ = mask_bgr.shape
    out = np.zeros((h, w), dtype=np.int64)
    for bgr, idx in BGR2IDX.items():
        match = np.all(mask_bgr == np.array(bgr, dtype=np.uint8)[None, None, :], axis=2)
        out[match] = idx
    return out

def index_to_rgb(idx_map: np.ndarray) -> np.ndarray:
    rgb = np.zeros((*idx_map.shape, 3), dtype=np.uint8)
    for k, rgbc in IDX2RGB.items():
        rgb[idx_map == k] = rgbc
    return rgb

def overlay_mask_on_image(img_gray: np.ndarray, idx_map: np.ndarray, alpha: float = 0.5) -> np.ndarray:
    """Returns RGB overlay; img_gray assumed 0..255."""
    if img_gray.ndim == 2:
        base = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2RGB)
    else:
        base = cv2.cvtColor(img_gray, cv2.COLOR_BGR2RGB)
    color = index_to_rgb(idx_map)
    overlay = cv2.addWeighted(base, 1.0, color, alpha, 0)
    return overlay

def show_triplet(img, mask_idx, pred_idx=None, title=""):
    plt.figure(figsize=(12,4))
    plt.subplot(1,3,1); plt.imshow(img, cmap='gray'); plt.title('Image'); plt.axis('off')
    plt.subplot(1,3,2); plt.imshow(index_to_rgb(mask_idx)); plt.title('GT'); plt.axis('off')
    if pred_idx is not None:
        plt.subplot(1,3,3); plt.imshow(index_to_rgb(pred_idx)); plt.title('Pred'); plt.axis('off')
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()

def list_pairs(img_dir, mask_dir, exts=(".png", ".jpg", ".jpeg")) -> List[Tuple[str,str]]:
    imgs = []
    for e in exts:
        imgs.extend(sorted(glob.glob(os.path.join(img_dir, f"*{e}"))))
    pairs = []
    for ip in imgs:
        name = os.path.splitext(os.path.basename(ip))[0]
        for e in exts:
            mp = os.path.join(mask_dir, name + e)
            if os.path.exists(mp):
                pairs.append((ip, mp))
                break
    return pairs

def build_transforms(img_size: int, is_train: bool):
    if is_train:
        return A.Compose([
            A.LongestMaxSize(max_size=img_size),
            A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),
            A.HorizontalFlip(p=0.5),
            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=10, border_mode=cv2.BORDER_CONSTANT, p=0.5),
            A.RandomBrightnessContrast(p=0.3),
            A.CLAHE(clip_limit=2.0, p=0.2),
            A.Normalize(mean=(0.5,), std=(0.5,)),   # grayscale normalization
            ToTensorV2()
        ])
    else:
        return A.Compose([
            A.LongestMaxSize(max_size=img_size),
            A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),
            A.Normalize(mean=(0.5,), std=(0.5,)),
            ToTensorV2()
        ])

class SegDataset(Dataset):
    def __init__(self, pairs: List[Tuple[str,str]], transform, read_gray=True):
        self.pairs = pairs
        self.tf = transform
        self.read_gray = read_gray
        # Precompute presence of small structures for oversampling
        self.contains_small = []
        for _, mp in self.pairs:
            m = cv2.imread(mp, cv2.IMREAD_UNCHANGED)
            if m is None:
                self.contains_small.append(False); continue
            idx = color_mask_to_index(m if m.ndim==3 else m)
            has_small = np.any(idx==2) or np.any(idx==3)
            self.contains_small.append(bool(has_small))

    def __len__(self): return len(self.pairs)

    def __getitem__(self, i):
        ip, mp = self.pairs[i]
        img = cv2.imread(ip, cv2.IMREAD_GRAYSCALE) if self.read_gray else cv2.imread(ip, cv2.IMREAD_COLOR)
        mask_raw = cv2.imread(mp, cv2.IMREAD_UNCHANGED)
        if mask_raw is None or img is None:
            raise FileNotFoundError(f"Missing image or mask at {ip} / {mp}")

        mask_idx = color_mask_to_index(mask_raw)

        # Albumentations expects HWC, mask as 2D
        img_hwc = img if img.ndim==3 else img[..., None]
        # For grayscale: Albumentations Normalize(mean=(0.5,), std=(0.5,)) expects 1-channel
        if img_hwc.shape[2] == 1:
            img_hwc = img_hwc  # H,W,1
        else:
            # If color ultrasound exists, convert to single channel now (optional)
            img_hwc = cv2.cvtColor(img_hwc, cv2.COLOR_BGR2GRAY)[..., None]

        augmented = self.tf(image=img_hwc, mask=mask_idx)
        img_t = augmented['image']  # shape: (C,H,W)
        mask_t = augmented['mask'].long()  # (H,W)
        # Ensure single channel input
        if img_t.shape[0] != 1:
            # move to 1C by taking channel 0 (or convert)
            img_t = img_t[:1, ...]
        return img_t, mask_t, os.path.basename(ip)

# Build file lists
img_dir = os.path.join(CFG.DATA_ROOT, CFG.IMG_DIR)
mask_dir = os.path.join(CFG.DATA_ROOT, CFG.MASK_DIR)
pairs_all = list_pairs(img_dir, mask_dir)
print(f"Found {len(pairs_all)} pairs")

# Train/val split (stratified by presence of small structures)
random.shuffle(pairs_all)
split = int(0.85 * len(pairs_all))
pairs_train, pairs_val = pairs_all[:split], pairs_all[split:]
print(f"Train: {len(pairs_train)} | Val: {len(pairs_val)}")

# Datasets
train_tf = build_transforms(CFG.img_size, is_train=True)
val_tf = build_transforms(CFG.img_size, is_train=False)
ds_train = SegDataset(pairs_train, transform=train_tf)
ds_val   = SegDataset(pairs_val,   transform=val_tf)

# Class frequency over TRAIN (for CE weights)
def compute_class_histogram(ds: SegDataset, n_classes: int) -> np.ndarray:
    counts = np.zeros(n_classes, dtype=np.int64)
    for _, mp in ds.pairs:
        m = cv2.imread(mp, cv2.IMREAD_UNCHANGED)
        if m is None: continue
        idx = color_mask_to_index(m)
        for c in range(n_classes):
            counts[c] += (idx==c).sum()
    return counts

class_counts = compute_class_histogram(ds_train, CFG.n_classes)
class_freq = class_counts / np.maximum(class_counts.sum(), 1)
inv_freq = 1.0 / np.clip(class_freq, 1e-8, None)
inv_freq = inv_freq / inv_freq.mean()  # normalize
# Optional smoothing to avoid extreme weights
ce_weights = (1-CFG.ce_weight_smoothing) * inv_freq + CFG.ce_weight_smoothing * np.ones_like(inv_freq)
print("Class counts:", class_counts)
print("CE weights (normalized):", ce_weights)

# Oversampling by image presence of CSP or LV
sample_weights = [CFG.oversample_factor if has else 1.0 for has in ds_train.contains_small]
sampler = WeightedRandomSampler(weights=torch.DoubleTensor(sample_weights),
                                num_samples=len(sample_weights), replacement=True)

# DataLoaders
loader_train = DataLoader(ds_train, batch_size=CFG.batch_size, sampler=sampler,
                          num_workers=CFG.num_workers, pin_memory=True)
loader_val   = DataLoader(ds_val, batch_size=CFG.batch_size, shuffle=False,
                          num_workers=CFG.num_workers, pin_memory=True)

import torch
import torch.nn as nn
import torch.nn.functional as F

class AttentionBlock(nn.Module):
    def __init__(self, F_g, F_l, F_int):
        super().__init__()
        self.W_g = nn.Sequential(
            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),
            nn.BatchNorm2d(F_int)
        )

        self.W_x = nn.Sequential(
            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),
            nn.BatchNorm2d(F_int)
        )

        self.psi = nn.Sequential(
            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),
            nn.BatchNorm2d(1),
            nn.Sigmoid()
        )

        self.relu = nn.ReLU(inplace=True)

    def forward(self, g, x):
        g1 = self.W_g(g)
        x1 = self.W_x(x)
        psi = self.relu(g1 + x1)
        psi = self.psi(psi)
        return x * psi

class DoubleConv(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
        )
    def forward(self, x): return self.block(x)

class Down(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.pool = nn.MaxPool2d(2)
        self.conv = DoubleConv(in_ch, out_ch)
    def forward(self, x): return self.conv(self.pool(x))

class Up(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, kernel_size=2, stride=2)
        self.att = AttentionBlock(F_g=in_ch//2, F_l=in_ch//2, F_int=in_ch//4)
        self.conv = DoubleConv(in_ch, out_ch)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        # Apply attention
        x2 = self.att(g=x1, x=x2)
        # Pad if needed
        diffY = x2.size(2) - x1.size(2)
        diffX = x2.size(3) - x1.size(3)
        x1 = nn.functional.pad(x1, [diffX//2, diffX - diffX//2,
                                  diffY//2, diffY - diffY//2])
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)

class UNet(nn.Module):
    def __init__(self, in_channels=1, n_classes=4, base=64):
        super().__init__()
        self.inc = DoubleConv(in_channels, base)
        self.down1 = Down(base, base*2)
        self.down2 = Down(base*2, base*4)
        self.down3 = Down(base*4, base*8)
        self.down4 = Down(base*8, base*8)
        self.up1 = Up(base*16, base*4)
        self.up2 = Up(base*8, base*2)
        self.up3 = Up(base*4, base)
        self.up4 = Up(base*2, base)
        self.outc = nn.Conv2d(base, n_classes, kernel_size=1)

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        x = self.up1(x5, x4)
        x = self.up2(x, x3)
        x = self.up3(x, x2)
        x = self.up4(x, x1)
        logits = self.outc(x)
        return logits

# Assuming CFG is defined elsewhere with in_channels and n_classes
model = UNet(in_channels=CFG.in_channels, n_classes=CFG.n_classes).to(device)
print(f"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M")

import torch
import torch.nn as nn
import torch.nn.functional as F

# --- Weighted Cross Entropy ---
ce_weight_t = torch.tensor(ce_weights, dtype=torch.float32, device=device)
ce_loss_fn = nn.CrossEntropyLoss(weight=ce_weight_t, ignore_index=-1)

# --- Focal Loss ---
class FocalLoss(nn.Module):
    def __init__(self, alpha=1.0, gamma=2.0, weight=None, ignore_index=-1):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.weight = weight
        self.ignore_index = ignore_index

    def forward(self, logits, target):
        # logits: (B,C,H,W), target: (B,H,W)
        ce_loss = F.cross_entropy(
            logits, target, weight=self.weight, ignore_index=self.ignore_index, reduction='none'
        )
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean()

focal_loss_fn = FocalLoss(alpha=1.0, gamma=2.0, weight=ce_weight_t, ignore_index=-1)

# --- Multi-class soft Dice loss ---
class SoftDiceLoss(nn.Module):
    def __init__(self, n_classes: int, class_weights: torch.Tensor = None, eps: float = 1e-6):
        super().__init__()
        self.n_classes = n_classes
        self.eps = eps
        if class_weights is None:
            class_weights = torch.ones(n_classes, dtype=torch.float32)
        self.register_buffer('w', class_weights / class_weights.mean())

    def forward(self, logits, target):
        # logits: (B,C,H,W), target: (B,H,W) int
        probs = torch.softmax(logits, dim=1)
        target_1h = torch.nn.functional.one_hot(
            torch.clamp(target, 0, self.n_classes-1), num_classes=self.n_classes
        ).permute(0,3,1,2).float()
        dims = (0,2,3)
        numer = (2 * (probs * target_1h).sum(dims) + self.eps)
        denom = (probs.pow(2).sum(dims) + target_1h.pow(2).sum(dims) + self.eps)
        dice_c = numer / denom  # (C,)
        return 1 - (dice_c * self.w).sum() / self.w.sum()

dice_w = torch.tensor([0.5, 1.0, 2.0, 2.0], dtype=torch.float32, device=device)  # emphasize CSP/LV
dice_loss_fn = SoftDiceLoss(CFG.n_classes, class_weights=dice_w)

# --- Combined Loss with Focal Loss ---
def combined_loss(logits, target, alpha=CFG.dice_weight, beta=CFG.focal_weight):
    ce_loss = ce_loss_fn(logits, target)
    focal_loss = focal_loss_fn(logits, target)
    dice_loss = dice_loss_fn(logits, target)
    return (1 - alpha - beta) * ce_loss + beta * focal_loss + alpha * dice_loss

# --- Metrics ---
@torch.no_grad()
def confusion_matrix(pred, target, n_classes):
    """
    pred, target: (B,H,W) int
    returns: (n_classes, n_classes) with rows=target, cols=pred
    """
    k = (target >= 0) & (target < n_classes)
    bins = target[k].to(torch.int64) * n_classes + pred[k].to(torch.int64)
    cm = torch.bincount(bins, minlength=n_classes**2).reshape(n_classes, n_classes)
    return cm

def iou_from_cm(cm: torch.Tensor):
    # IoU per class = TP / (TP + FP + FN)
    tp = cm.diagonal()
    fp = cm.sum(0) - tp
    fn = cm.sum(1) - tp
    denom = tp + fp + fn
    iou = (tp / torch.clamp(denom, min=1)).cpu().numpy()
    return iou

def dice_from_cm(cm: torch.Tensor):
    tp = cm.diagonal()
    fp = cm.sum(0) - tp
    fn = cm.sum(1) - tp
    dice = (2*tp / torch.clamp(2*tp + fp + fn, min=1)).cpu().numpy()
    return dice

print(f"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M")

def metric_score(iou: np.ndarray, weights: Dict[int,float]) -> float:
    """Weighted mean over classes for checkpointing (emphasize CSP/LV)."""
    num = sum(iou[c]*w for c, w in weights.items())
    den = sum(w for w in weights.values())
    return float(num / max(den, 1e-8))

# --- simple inference post-proc (parenchyma clean + constrain CSP/LV) ---
import numpy as np, cv2
def _keep_largest(bin_mask):
    n, lab, stats, _ = cv2.connectedComponentsWithStats(bin_mask.astype(np.uint8), 8)
    if n <= 1: return bin_mask.astype(bool)
    largest = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])
    return (lab == largest)

def postprocess_pred(pred, dilate_px=10, par_kernel=7, small_min_area=15, small_keep_k=2):
    out = pred.copy()
    par = (out==1).astype(np.uint8)
    par = cv2.morphologyEx(par, cv2.MORPH_CLOSE, np.ones((par_kernel,par_kernel), np.uint8))
    par = _keep_largest(par).astype(np.uint8)
    out[out==1] = 0; out[par>0] = 1
    par_dil = cv2.dilate(par, np.ones((dilate_px,dilate_px), np.uint8), 1).astype(bool)
    for c in (2,3):
        cls = (out==c) & par_dil
        n, lab, stats, _ = cv2.connectedComponentsWithStats(cls.astype(np.uint8), 8)
        keep = np.zeros_like(cls, bool)
        if n > 1:
            areas = stats[1:, cv2.CC_STAT_AREA]
            order = np.argsort(-areas)
            kept = 0
            for j in order:
                if areas[j] < small_min_area:
                    break
                keep |= (lab == (j+1))
                kept += 1
                if small_keep_k is not None and kept >= small_keep_k:
                    break
        # rewrite class c using kept blobs
        out[out==c] = 0
        out[keep]   = c

    return out

def validate(model, loader, n_classes):
    model.eval()
    total_loss = 0.0
    cm_total = torch.zeros((n_classes, n_classes), dtype=torch.int64, device='cpu')

    with torch.no_grad():
        for imgs, masks, _ in loader:
            imgs = imgs.to(device, non_blocking=True).float()
            masks = masks.to(device, non_blocking=True).long()
            logits = model(imgs)
            total_loss += combined_loss(logits, masks).item() * imgs.size(0)

            preds = torch.argmax(logits, dim=1)

            preds_np = preds.cpu().numpy()
            for i in range(preds_np.shape[0]):
                preds_np[i] = postprocess_pred(preds_np[i])
            preds_pp = torch.from_numpy(preds_np.astype(np.int64))  # ensure int64


            cm = confusion_matrix(preds_pp, masks.cpu(), n_classes)
            cm_total += cm

    avg_loss = total_loss / len(loader.dataset)
    iou  = iou_from_cm(cm_total)
    dice = dice_from_cm(cm_total)
    return avg_loss, iou, dice


def train(model, loader_train, loader_val, epochs=CFG.epochs, lr=CFG.lr):
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=CFG.weight_decay)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)

    from torch import amp
    scaler = amp.GradScaler('cuda', enabled=CFG.amp)

    history = {'train_loss':[], 'val_loss':[], 'val_iou':[], 'val_dice':[], 'lr':[]}
    best_score = -1.0
    best_path = os.path.join(CFG.OUT_DIR, "best_unet_old.pt")

    for epoch in range(1, epochs+1):
        model.train()
        running = 0.0
        t0 = time.time()

        for imgs, masks, _ in loader_train:
            imgs  = imgs.to(device, non_blocking=True).float()
            masks = masks.to(device, non_blocking=True).long()

            optimizer.zero_grad(set_to_none=True)
            with amp.autocast('cuda', enabled=CFG.amp):
                logits = model(imgs)
                loss   = combined_loss(logits, masks)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            running += loss.item() * imgs.size(0)

        train_loss = running / len(loader_train.dataset)
        val_loss, val_iou, val_dice = validate(model, loader_val, CFG.n_classes)

        # focus score on CSP/LV
        score   = metric_score(val_iou, CFG.ckpt_metric_weights)
        prev_lr = optimizer.param_groups[0]['lr']
        scheduler.step(score)
        curr_lr = optimizer.param_groups[0]['lr']
        if curr_lr < prev_lr:
            print(f"  -> LR reduced: {prev_lr:.2e} → {curr_lr:.2e}")

        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['val_iou'].append(val_iou.tolist())
        history['val_dice'].append(val_dice.tolist())
        history['lr'].append(curr_lr)

        dt = time.time() - t0
        print(f"[{epoch:03d}/{epochs}] train {train_loss:.4f} | val {val_loss:.4f} | "
              f"IoU bg/par/CSP/LV: {val_iou.round(3)} | score(CSP+LV-weighted)={score:.4f} | "
              f"lr={curr_lr:.2e} | {dt:.1f}s")

        if score > best_score:
            best_score = score
            torch.save({'model': model.state_dict(),
                        'cfg': CFG.__dict__,
                        'score': best_score,
                        'epoch': epoch}, best_path)
            print(f"  -> saved new best to {best_path} (score={best_score:.4f})")

    return history, best_path, best_score

history, best_path, best_score = train(model, loader_train, loader_val)
print("Best model:", best_path, "score:", best_score)

"""if stopped mid way"""

import matplotlib.pyplot as plt
import numpy as np

def plot_history(history):
    if not history or not history.get('train_loss'):
        print("Warning: History is empty or missing train_loss. Skipping plotting.")
        return

    epochs = range(1, len(history['train_loss']) + 1)
    labels = ['BG', 'Parenchyma', 'CSP', 'LV']

    # Use a single figure with subplots for compactness
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    fig.suptitle('Training and Validation Metrics', fontsize=16)

    # 1. Loss Plot
    axes[0, 0].plot(epochs, history['train_loss'], label='Train Loss')
    axes[0, 0].plot(epochs, history['val_loss'], label='Val Loss')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)

    # 2. IoU for CSP & LV
    ious = np.array(history['val_iou'])  # shape (E, C)
    if ious.shape[1] != len(labels):
        print(f"Warning: IoU shape {ious.shape} does not match {len(labels)} classes.")
        return
    axes[0, 1].plot(epochs, ious[:, 2], label='CSP IoU')
    axes[0, 1].plot(epochs, ious[:, 3], label='LV IoU')
    axes[0, 1].set_ylim(0, 1)
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('IoU')
    axes[0, 1].set_title('IoU - Small Structures (CSP & LV)')
    axes[0, 1].legend()
    axes[0, 1].grid(True)

    # 3. Dice for CSP & LV (New)
    dices = np.array(history['val_dice'])  # shape (E, C)
    if dices.shape[1] != len(labels):
        print(f"Warning: Dice shape {dices.shape} does not match {len(labels)} classes.")
        return
    axes[1, 0].plot(epochs, dices[:, 2], label='CSP Dice')
    axes[1, 0].plot(epochs, dices[:, 3], label='LV Dice')
    axes[1, 0].set_ylim(0, 1)
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Dice')
    axes[1, 0].set_title('Dice - Small Structures (CSP & LV)')
    axes[1, 0].legend()
    axes[1, 0].grid(True)

    # 4. Learning Rate (New)
    axes[1, 1].plot(epochs, history['lr'], label='Learning Rate')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Learning Rate')
    axes[1, 1].set_title('Learning Rate Schedule')
    axes[1, 1].set_yscale('log')  # Log scale for better visibility
    axes[1, 1].legend()
    axes[1, 1].grid(True)

    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit title
    plt.show()

    # Optional: Per-class IoU plots (as in original)
    for c in range(ious.shape[1]):
        plt.figure(figsize=(6, 4))
        plt.plot(epochs, ious[:, c])
        plt.ylim(0, 1)
        plt.xlabel('Epoch')
        plt.ylabel('IoU')
        plt.title(f'IoU - {labels[c]}')
        plt.grid(True)
        plt.show()

plot_history(history)

import os
import torch
import numpy as np

# Load best model with error handling
best_path = os.path.join(CFG.OUT_DIR, "best_unet_old.pt")
if not os.path.exists(best_path):
    print(f"Warning: Best checkpoint not found at {best_path}")
    # Fallback to last checkpoint or initialize randomly
    best_path = os.path.join(CFG.OUT_DIR, "last_unet_old.pt")  # Optional fallback
else:
    print(f"Loading best model from {best_path}")

try:
    ckpt = torch.load(best_path, map_location=device)
    model.load_state_dict(ckpt['model'])
    print(f"Loaded model with score: {ckpt.get('score', 'N/A'):.4f} at epoch {ckpt.get('epoch', 'N/A')}")
except Exception as e:
    print(f"Error loading checkpoint: {e}")
    # Handle error (e.g., reinitialize model or use fallback)

model.eval()

# Final metrics (uncomment to evaluate):
# val_loss, val_iou, val_dice = validate(model, loader_val, CFG.n_classes)
# print("Final Val Loss:", round(val_loss,4))
# print("Final IoU bg/par/CSP/LV:", np.round(val_iou, 4))
# print("Final Dice bg/par/CSP/LV:", np.round(val_dice, 4))
# print("CSP/LV Weighted Score:", metric_score(val_iou, CFG.ckpt_metric_weights))

# Visualize a few validation samples (prioritize ones with CSP/LV)
samples = []
for i, (img, m, name) in enumerate(ds_val):
    has_small = bool((m==2).any() or (m==3).any())
    # Bonus for images with both CSP and LV
    has_both = bool((m==2).any() and (m==3).any())
    priority = 2 if has_both else (1 if has_small else 0)
    samples.append((priority, i))
samples = [i for _, i in sorted(samples, reverse=True)[:6]]  # top 6 with small structures first
print(f"Visualizing samples with CSP/LV priority: indices {samples}")

@torch.no_grad()
def predict_batch(imgs):
    """Predict batch with optional post-processing."""
    imgs = imgs.to(device, non_blocking=True).float()
    logits = model(imgs)
    preds = torch.argmax(logits, dim=1)

    # Optional: Apply post-processing (uncomment if desired)
    # preds_np = preds.cpu().numpy()
    # for i in range(preds_np.shape[0]):
    #     preds_np[i] = postprocess_pred(preds_np[i])
    # preds = torch.from_numpy(preds_np.astype(np.int64))

    return preds.cpu()

# Visualize samples
fig_count = 0
for idx in samples:
    try:
        img_t, m_t, name = ds_val[idx]
        pred = predict_batch(img_t.unsqueeze(0))[0]

        # For display, bring image back to 0..255 (assuming [-1,1] normalization)
        img_np = (img_t[0].cpu().numpy() * 0.5 + 0.5) * 255.0
        img_np = np.clip(img_np, 0, 255).astype(np.uint8)

        # Call your visualization function
        show_triplet(img_np, m_t.numpy(), pred.numpy(), title=name)
        fig_count += 1
    except Exception as e:
        print(f"Error visualizing sample {idx}: {e}")
        continue

print(f"Visualized {fig_count} samples successfully.")

import os
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
import albumentations as A
from albumentations.pytorch import ToTensorV2

# --- Paths ---
img_path = "/content/drive/MyDrive/thesis/fetal_brain_project/data/test/Patient01791_Plane3_2_of_3_image.png"
mask_path = "/content/drive/MyDrive/thesis/fetal_brain_project/data/test/Patient01791_Plane3_2_of_3_mask.png"
best_path = "/content/drive/MyDrive/thesis/thesisv2/outputs/best_unet_old.pt"

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# --- Color/Index Helpers ---
BGR2IDX = {(0, 0, 0): 0, (0, 0, 255): 1, (0, 255, 0): 2, (255, 0, 0): 3}
def color_mask_to_index(m):
    if m.ndim == 2:
        return np.clip(m.astype(np.int64), 0, 3)
    out = np.zeros(m.shape[:2], np.int64)
    for bgr, idx in BGR2IDX.items():
        out[np.all(m == np.array(bgr, np.uint8), axis=2)] = idx
    return out

def index_to_rgb(idx):
    colors = np.array([[0, 0, 0], [255, 0, 0], [0, 255, 0], [0, 0, 255]], np.uint8)
    return colors[idx]

# --- Load Model ---
try:
    if not os.path.exists(best_path):
        raise FileNotFoundError(f"Checkpoint not found at {best_path}")
    model = UNet(in_channels=CFG.in_channels, n_classes=CFG.n_classes).to(device)
    ckpt = torch.load(best_path, map_location=device)
    model.load_state_dict(ckpt['model'])
    model.eval()
    print(f"Loaded model from {best_path}, score: {ckpt.get('score', 'N/A'):.4f}")
except Exception as e:
    print(f"Error loading model: {e}")
    exit(1)

# --- Preprocessing ---
tf = A.Compose([
    A.LongestMaxSize(max_size=CFG.img_size),
    A.PadIfNeeded(CFG.img_size, CFG.img_size, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),
    A.Normalize(mean=(0.5,), std=(0.5,)),
    ToTensorV2()
])

# --- Read and Transform ---
try:
    img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    mask_raw = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)
    if img_gray is None or mask_raw is None:
        raise FileNotFoundError("Failed to load image or mask")
    mask_idx = color_mask_to_index(mask_raw)
    aug = tf(image=img_gray[..., None], mask=mask_idx)
    x = aug['image'].unsqueeze(0).to(device, non_blocking=True).float()  # (1,1,H,W)
    gt = aug['mask'].cpu().numpy()  # (H,W)
except Exception as e:
    print(f"Error processing image/mask: {e}")
    exit(1)

# --- Inference ---
with torch.no_grad():
    logits = model(x)  # (1,C,H,W)
probs = torch.softmax(logits, dim=1)[0].cpu().numpy()  # (C,H,W)
pred_raw = torch.argmax(logits, dim=1)[0].cpu().numpy()  # (H,W)

# --- Post-Processing ---
def refine_lv_components(pred, probs=None, min_area=20, margin=0.06, keep_k=None):
    """Keep LV components if area>=min_area OR mean(p_lv - p_other)>=margin."""
    lv = (pred == 3).astype(np.uint8)
    n, labels, stats, _ = cv2.connectedComponentsWithStats(lv, 8)
    if n <= 1:
        return pred
    comp_ids = np.arange(1, n)
    areas = stats[1:, cv2.CC_STAT_AREA]
    ok = areas >= min_area
    if probs is not None:
        p_lv = probs[3]
        p_oth = probs[[0, 1, 2]].max(axis=0)
        for lab in comp_ids[~ok]:
            comp = (labels == lab)
            if comp.any() and (p_lv[comp] - p_oth[comp]).mean() >= margin:
                ok[lab - 1] = True
    keep_ids = comp_ids[ok]
    if keep_k is not None and len(keep_ids) > keep_k:
        order = np.argsort(-areas[ok])[:keep_k]
        keep_ids = keep_ids[order]
    keep = np.isin(labels, keep_ids)
    out = pred.copy()
    out[pred == 3] = 0
    out[keep] = 3
    return out

# Apply general post-processing (from earlier code)
def postprocess_pred(pred, dilate_px=10, par_kernel=7, small_min_area=15, small_keep_k=2):
    out = pred.copy()
    par = (out == 1).astype(np.uint8)
    par = cv2.morphologyEx(par, cv2.MORPH_CLOSE, np.ones((par_kernel, par_kernel), np.uint8))
    par = _keep_largest(par).astype(np.uint8)
    out[out == 1] = 0
    out[par > 0] = 1
    par_dil = cv2.dilate(par, np.ones((dilate_px, dilate_px), np.uint8), 1).astype(bool)
    for c in (2, 3):
        cls = (out == c) & par_dil
        n, lab, stats, _ = cv2.connectedComponentsWithStats(cls.astype(np.uint8), 8)
        keep = np.zeros_like(cls, bool)
        if n > 1:
            areas = stats[1:, cv2.CC_STAT_AREA]
            order = np.argsort(-areas)
            kept = 0
            for j in order:
                if areas[j] < small_min_area:
                    break
                keep |= (lab == (j + 1))
                kept += 1
                if small_keep_k is not None and kept >= small_keep_k:
                    break
        out[out == c] = 0
        out[keep] = c
    return out

def _keep_largest(bin_mask):
    n, lab, stats, _ = cv2.connectedComponentsWithStats(bin_mask.astype(np.uint8), 8)
    if n <= 1:
        return bin_mask.astype(bool)
    largest = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])
    return (lab == largest)

# Apply post-processing
pred_proc = postprocess_pred(pred_raw)  # General post-processing
pred_ref = refine_lv_components(pred_proc, probs=probs, min_area=20, margin=0.06, keep_k=None)  # LV-specific refinement

# --- Metrics ---
def iou_dice(gt, pr, cls=3):
    g = (gt == cls)
    p = (pr == cls)
    inter = (g & p).sum()
    uni = (g | p).sum()
    dice = (2 * inter) / max(g.sum() + p.sum(), 1)
    iou = inter / max(uni, 1)
    return iou, dice

# Compute metrics for all classes
labels = ['BG', 'Parenchyma', 'CSP', 'LV']
for cls in range(CFG.n_classes):
    iou_raw, dice_raw = iou_dice(gt, pred_raw, cls)
    iou_proc, dice_proc = iou_dice(gt, pred_proc, cls)
    iou_ref, dice_ref = iou_dice(gt, pred_ref, cls)
    print(f"{labels[cls]}:")
    print(f"  Raw   -> IoU {iou_raw:.3f}, Dice {dice_raw:.3f}")
    print(f"  Proc  -> IoU {iou_proc:.3f}, Dice {dice_proc:.3f}")
    print(f"  Refin -> IoU {iou_ref:.3f}, Dice {dice_ref:.3f}")

# Compute loss for the sample
with torch.no_grad():
    sample_loss = combined_loss(logits, torch.from_numpy(gt).unsqueeze(0).to(device).long())
    print(f"Sample Loss: {sample_loss.item():.4f}")

# --- Visualization ---
img_disp = (x[0, 0].cpu().numpy() * 0.5 + 0.5) * 255.0
img_disp = np.clip(img_disp, 0, 255).astype(np.uint8)
pred_int = pred_ref.astype(np.int64)
gt_int = gt.astype(np.int64)
mismatch = (pred_int != gt_int).astype(np.uint8) * 255
frac = float((pred_int != gt_int).mean())

plt.figure(figsize=(14, 3.8))
plt.subplot(1, 4, 1)
plt.imshow(img_disp, cmap='gray')
plt.title("Ultrasound")
plt.axis('off')
plt.subplot(1, 4, 2)
plt.imshow(index_to_rgb(gt_int))
plt.title("Ground Truth")
plt.axis('off')
plt.subplot(1, 4, 3)
plt.imshow(index_to_rgb(pred_int))
plt.title("Prediction (Refined)")
plt.axis('off')
plt.subplot(1, 4, 4)
plt.imshow(mismatch, cmap='gray')
plt.title(f"Mismatch Heatmap\nFrac: {frac:.3f}")
plt.axis('off')
plt.tight_layout()
plt.savefig("test_visualization.png")  # Save figure
plt.show()

# # Save a full trace (optional)
# example = torch.randn(1, CFG.in_channels, CFG.img_size, CFG.img_size, device=device)
# traced = torch.jit.trace(model, example)
# traced_path = os.path.join(CFG.OUT_DIR, "unet_traced.pt")
# traced.save(traced_path)
# print("Saved traced model:", traced_path)

# # In a new session, to reload best model:
# # model = UNet(in_channels=CFG.in_channels, n_classes=CFG.n_classes).to(device)
# # ckpt = torch.load(best_path, map_location=device)
# # model.load_state_dict(ckpt['model'])
# # model.eval()

# Save a full trace (optional)
example = torch.randn(1, CFG.in_channels, CFG.img_size, CFG.img_size, device=device)
traced = torch.jit.trace(model, example)
traced_path = os.path.join(CFG.OUT_DIR, "unet_traced_old.pt")
traced.save(traced_path)
print("Saved traced model:", traced_path)

# In a new session, to reload best model:
# model = UNet(in_channels=CFG.in_channels, n_classes=CFG.n_classes).to(device)
# ckpt = torch.load(best_path, map_location=device)
# model.load_state_dict(ckpt['model'])
# model.eval()

# # Verify LV detection + get per-image metrics
# import numpy as np, cv2, matplotlib.pyplot as plt

# lv_gt    = (gt==3).sum()
# lv_pred  = (pred==3).sum()
# lv_inter = ((gt==3) & (pred.cpu().numpy()==3)).sum()
# lv_union = ((gt==3) | (pred.cpu().numpy()==3)).sum()

# lv_iou  = (lv_inter / lv_union) if lv_union else 0.0
# lv_dice = (2*lv_inter / (lv_gt + lv_pred)) if (lv_gt + lv_pred) else 0.0

# print(f"LV pixels — GT: {lv_gt}, Pred: {lv_pred}, Intersect: {lv_inter}")
# print(f"LV IoU: {lv_iou:.3f} | LV Dice: {lv_dice:.3f}")

# # Visual sanity: outline Pred LV (blue) vs GT LV (green)
# pred_edge = cv2.Canny(((pred.cpu().numpy()==3).astype(np.uint8)*255), 0, 1)
# gt_edge   = cv2.Canny(((gt==3).astype(np.uint8)*255),   0, 1)
# overlay   = cv2.cvtColor(img_disp, cv2.COLOR_GRAY2RGB)
# overlay[pred_edge>0] = [0,0,255]   # blue = Pred LV
# overlay[gt_edge>0]   = [0,255,0]   # green = GT LV

# plt.figure(figsize=(5,5))
# plt.imshow(overlay); plt.axis('off')
# plt.title(f"LV Pred (blue) vs GT (green)\nIoU={lv_iou:.3f}, Dice={lv_dice:.3f}")
# plt.show()

plt.imshow(index_to_rgb(gt)); plt.title("Ground Truth"); plt.axis('off'); plt.show()

import numpy as np
import cv2
import matplotlib.pyplot as plt

# Assuming pred is from inference: pred = torch.argmax(logits, dim=1)[0].cpu().numpy()
pred_np = pred.cpu().numpy()  # Convert once to avoid repeated conversions

# Verify LV detection + get per-image metrics
lv_gt = (gt == 3).sum()
lv_pred = (pred_np == 3).sum()
lv_inter = ((gt == 3) & (pred_np == 3)).sum()
lv_union = ((gt == 3) | (pred_np == 3)).sum()

lv_iou = (lv_inter / lv_union) if lv_union else 0.0
lv_dice = (2 * lv_inter / (lv_gt + lv_pred)) if (lv_gt + lv_pred) else 0.0

print(f"LV pixels — GT: {lv_gt}, Pred: {lv_pred}, Intersect: {lv_inter}")
print(f"LV IoU: {lv_iou:.3f} | LV Dice: {lv_dice:.3f}")

# Apply post-processing (using refine_lv_safe from previous code)
def refine_lv_safe_with_probs(pred, probs, keep_k=2, min_area=5, margin=0.06, morph='close', ksize=3):
    lv = (pred == 3).astype(np.uint8)
    if morph == 'close':
        lv = cv2.morphologyEx(lv, cv2.MORPH_CLOSE, np.ones((ksize, ksize), np.uint8))
    n, labels, stats, _ = cv2.connectedComponentsWithStats(lv, 8)
    if n <= 1:
        return (pred == 3)

    areas = stats[1:, cv2.CC_STAT_AREA]
    p_lv = probs[3]
    p_oth = probs[[0, 1, 2]].max(axis=0)
    comp_ids = np.arange(1, n)
    ok = areas >= min_area
    for lab in comp_ids[~ok]:
        comp = (labels == lab)
        if comp.any() and (p_lv[comp] - p_oth[comp]).mean() >= margin:
            ok[lab - 1] = True
    keep_ids = comp_ids[ok][:keep_k]
    keep = np.isin(labels, keep_ids)
    return keep if keep.any() else (pred == 3)

lv_ref = refine_lv_safe_with_probs(pred_np, probs, keep_k=3, min_area=3, margin=0.04, morph='open', ksize=3)
pred_ref = pred_np.copy()
pred_ref[pred_np == 3] = 0
pred_ref[lv_ref] = 3


pred_ref = pred_np.copy()
lv_ref = refine_lv_safe(pred_np, keep_k=2, min_area=5, morph='close', ksize=3)
pred_ref[pred_np == 3] = 0
pred_ref[lv_ref] = 3

# Recalculate metrics with refined prediction
lv_ref_inter = ((gt == 3) & lv_ref).sum()
lv_ref_union = ((gt == 3) | lv_ref).sum()
lv_ref_pred = lv_ref.sum()

lv_ref_iou = (lv_ref_inter / lv_ref_union) if lv_ref_union else 0.0
lv_ref_dice = (2 * lv_ref_inter / (lv_gt + lv_ref_pred)) if (lv_gt + lv_ref_pred) else 0.0

print(f"LV refined pixels — Pred: {lv_ref_pred}, Intersect: {lv_ref_inter}")
print(f"LV refined IoU: {lv_ref_iou:.3f} | LV refined Dice: {lv_ref_dice:.3f}")

# Visual sanity: outline Pred LV (blue) vs GT LV (green)
pred_edge = cv2.Canny((pred_np == 3).astype(np.uint8) * 255, 10, 20)  # Adjusted thresholds
gt_edge = cv2.Canny((gt == 3).astype(np.uint8) * 255, 10, 20)
overlay = cv2.cvtColor(img_disp, cv2.COLOR_GRAY2RGB)
overlay[pred_edge > 0] = [0, 0, 255]  # blue = Pred LV
overlay[gt_edge > 0] = [0, 255, 0]   # green = GT LV

plt.figure(figsize=(5, 5))
plt.imshow(overlay)
plt.axis('off')
plt.title(f"LV Pred (blue) vs GT (green)\nRaw IoU={lv_iou:.3f}, Dice={lv_dice:.3f}\nRefined IoU={lv_ref_iou:.3f}, Dice={lv_ref_dice:.3f}")
plt.show()

# Optional: Visualize raw and refined predictions
plt.figure(figsize=(10, 5))
plt.subplot(1, 3, 1); plt.imshow(index_to_rgb(gt)); plt.title("Ground Truth"); plt.axis('off')
plt.subplot(1, 3, 2); plt.imshow(index_to_rgb(pred_np)); plt.title("Raw Prediction"); plt.axis('off')
plt.subplot(1, 3, 3); plt.imshow(index_to_rgb(pred_ref)); plt.title("Refined Prediction"); plt.axis('off')
plt.tight_layout()
plt.show()

# import numpy as np, cv2, matplotlib.pyplot as plt

# def refine_lv(pred, keep_k=1, min_area=20, restrict_to_parenchyma=True):
#     # Convert tensor to numpy array
#     pred_np = pred.cpu().numpy()

#     lv = (pred_np==3).astype(np.uint8)
#     if restrict_to_parenchyma:
#         lv = cv2.bitwise_and(lv, (pred_np==1).astype(np.uint8))  # keep LV only inside predicted parenchyma
#     lv = cv2.morphologyEx(lv, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))

#     n, labels, stats, _ = cv2.connectedComponentsWithStats(lv, connectivity=8)
#     if n <= 1:  # no LV found
#         return lv.astype(bool)

#     # keep the top-k largest components above min_area
#     areas = stats[1:, cv2.CC_STAT_AREA]
#     order = [i for i in np.argsort(-areas) if areas[i] >= min_area][:keep_k]
#     keep = np.isin(labels, np.array(order)+1)
#     return keep

# ref = refine_lv(pred, keep_k=1, min_area=30, restrict_to_parenchyma=True)

# gt_lv   = (gt==3)
# pred_lv = ref

# inter = np.logical_and(gt_lv, pred_lv).sum()
# union = np.logical_or(gt_lv, pred_lv).sum()
# iou  = inter / union if union else 0.0
# dice = (2*inter) / (gt_lv.sum() + pred_lv.sum()) if (gt_lv.sum()+pred_lv.sum()) else 0.0
# print("Refined LV IoU:", round(iou,3), "Dice:", round(dice,3))

# # overlay: refined pred (blue) vs GT (green)
# pe = cv2.Canny(pred_lv.astype(np.uint8)*255, 0, 1)
# ge = cv2.Canny(gt_lv.astype(np.uint8)*255,   0, 1)
# overlay = cv2.cvtColor(img_disp, cv2.COLOR_GRAY2RGB)
# overlay[pe>0] = [0,0,255]
# overlay[ge>0] = [0,255,0]
# plt.figure(figsize=(5,5)); plt.imshow(overlay); plt.axis('off')
# plt.title(f"Refined LV (blue) vs GT (green)\nIoU={iou:.3f}, Dice={dice:.3f}")
# plt.show()

import numpy as np, cv2, matplotlib.pyplot as plt

def refine_lv(pred, keep_k=1, min_area=20, restrict_to_parenchyma=True):
    # Convert tensor to numpy array
    pred_np = pred.cpu().numpy()

    lv = (pred_np==3).astype(np.uint8)
    if restrict_to_parenchyma:
        lv = cv2.bitwise_and(lv, (pred_np==1).astype(np.uint8))  # keep LV only inside predicted parenchyma
    lv = cv2.morphologyEx(lv, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))

    n, labels, stats, _ = cv2.connectedComponentsWithStats(lv, connectivity=8)
    if n <= 1:  # no LV found
        return lv.astype(bool)

    # keep the top-k largest components above min_area
    areas = stats[1:, cv2.CC_STAT_AREA]
    order = [i for i in np.argsort(-areas) if areas[i] >= min_area][:keep_k]
    keep = np.isin(labels, np.array(order)+1)
    return keep

ref = refine_lv(pred, keep_k=1, min_area=30, restrict_to_parenchyma=True)

gt_lv   = (gt==3)
pred_lv = ref

inter = np.logical_and(gt_lv, pred_lv).sum()
union = np.logical_or(gt_lv, pred_lv).sum()
iou  = inter / union if union else 0.0
dice = (2*inter) / (gt_lv.sum() + pred_lv.sum()) if (gt_lv.sum()+pred_lv.sum()) else 0.0
print("Refined LV IoU:", round(iou,3), "Dice:", round(dice,3))

# overlay: refined pred (blue) vs GT (green)
pe = cv2.Canny(pred_lv.astype(np.uint8)*255, 0, 1)
ge = cv2.Canny(gt_lv.astype(np.uint8)*255,   0, 1)
overlay = cv2.cvtColor(img_disp, cv2.COLOR_GRAY2RGB)
overlay[pe>0] = [0,0,255]
overlay[ge>0] = [0,255,0]
plt.figure(figsize=(5,5)); plt.imshow(overlay); plt.axis('off')
plt.title(f"Refined LV (blue) vs GT (green)\nIoU={iou:.3f}, Dice={dice:.3f}")
plt.show()

# # 1) Reload model with the cfg stored in the checkpoint
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# ckpt = torch.load(best_path, map_location=device)
# cfg_tr = ckpt.get('cfg', {})  # contains in_channels, n_classes, img_size, etc.

# model = UNet(in_channels=cfg_tr.get('in_channels',1),
#              n_classes=cfg_tr.get('n_classes',4)).to(device)
# model.load_state_dict(ckpt['model']); model.eval()

# # 2) Use the same normalization as validation (match your earlier code)
# import albumentations as A
# from albumentations.pytorch import ToTensorV2
# tf = A.Compose([
#     A.LongestMaxSize(max_size=cfg_tr.get('img_size', 512)),
#     A.PadIfNeeded(cfg_tr.get('img_size', 512), cfg_tr.get('img_size', 512),
#                   border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),
#     A.Normalize(mean=(0.5,), std=(0.5,)),
#     ToTensorV2()
# ])

# # ...run inference to get `pred` and `gt`...

# # 3) Sanity check: did the model predict any LV pixels?
# vals, cnts = np.unique(pred, return_counts=True)
# print(dict(zip(vals, cnts)))        # look for key 3
# print("LV pixels predicted:", (pred==3).sum())

# 1) Reload model with the cfg stored in the checkpoint
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
ckpt = torch.load(best_path, map_location=device)
cfg_tr = ckpt.get('cfg', {})  # contains in_channels, n_classes, img_size, etc.

model = UNet(in_channels=cfg_tr.get('in_channels',1),
             n_classes=cfg_tr.get('n_classes',4)).to(device)
model.load_state_dict(ckpt['model']); model.eval()

# 2) Use the same normalization as validation (match your earlier code)
import albumentations as A
from albumentations.pytorch import ToTensorV2
tf = A.Compose([
    A.LongestMaxSize(max_size=cfg_tr.get('img_size', 512)),
    A.PadIfNeeded(cfg_tr.get('img_size', 512), cfg_tr.get('img_size', 512),
                  border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),
    A.Normalize(mean=(0.5,), std=(0.5,)),
    ToTensorV2()
])

# ...run inference to get `pred` and `gt`...

# 3) Sanity check: did the model predict any LV pixels?
vals, cnts = np.unique(pred, return_counts=True)
print(dict(zip(vals, cnts)))        # look for key 3
print("LV pixels predicted:", (pred==3).sum())

import os
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
import albumentations as A
from albumentations.pytorch import ToTensorV2

# --- Paths and Setup ---
ckpt_path = "/content/drive/MyDrive/thesis/thesisv2/outputs/best_unet_old.pt"
img_path = "/content/drive/MyDrive/thesis/fetal_brain_project/data/test/Patient01791_Plane3_2_of_3_image.png"
mask_path = "/content/drive/MyDrive/thesis/fetal_brain_project/data/test/Patient01791_Plane3_2_of_3_mask.png"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# --- Color/Index Helpers ---
BGR2IDX = {(0, 0, 0): 0, (0, 0, 255): 1, (0, 255, 0): 2, (255, 0, 0): 3}
def color_mask_to_index(m):
    if m.ndim == 2:
        return np.clip(m.astype(np.int64), 0, 3)
    out = np.zeros(m.shape[:2], np.int64)
    for bgr, idx in BGR2IDX.items():
        out[np.all(m == np.array(bgr, np.uint8), axis=2)] = idx
    return out

def index_to_rgb(idx):
    colors = np.array([[0, 0, 0], [255, 0, 0], [0, 255, 0], [0, 0, 255]], np.uint8)
    return colors[idx]

# --- Load Model ---
try:
    model = UNet(in_channels=CFG.in_channels, n_classes=CFG.n_classes).to(device)
    ckpt = torch.load(ckpt_path, map_location=device)
    model.load_state_dict(ckpt['model'])
    model.eval()
    print(f"Loaded model from {ckpt_path}, score: {ckpt.get('score', 'N/A'):.4f}")
except Exception as e:
    print(f"Error loading model: {e}")
    exit(1)

# --- Preprocessing ---
tf = A.Compose([
    A.LongestMaxSize(max_size=CFG.img_size),
    A.PadIfNeeded(CFG.img_size, CFG.img_size, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),
    A.Normalize(mean=(0.5,), std=(0.5,)),
    ToTensorV2()
])

# --- Read and Transform ---
try:
    img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    mask_raw = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)
    if img_gray is None or mask_raw is None:
        raise FileNotFoundError("Failed to load image or mask")
    mask_idx = color_mask_to_index(mask_raw)
    aug = tf(image=img_gray[..., None], mask=mask_idx)
    x = aug['image'].unsqueeze(0).to(device, non_blocking=True).float()
    gt = aug['mask'].cpu().numpy()
except Exception as e:
    print(f"Error processing image/mask: {e}")
    exit(1)

# --- Inference ---
with torch.no_grad():
    logits = model(x)
probs = torch.softmax(logits, dim=1)[0].cpu().numpy()
pred_raw = torch.argmax(logits, dim=1)[0].cpu().numpy()  # Convert tensor to NumPy here

# --- Post-Processing ---
def refine_lv_safe(pred, keep_k=2, min_area=5, morph='close', ksize=3):
    """Keep LV components with area >= min_area, up to keep_k components."""
    lv = (pred == 3).astype(np.uint8)
    if morph == 'close':
        lv = cv2.morphologyEx(lv, cv2.MORPH_CLOSE, np.ones((ksize, ksize), np.uint8))
    elif morph == 'open':
        lv = cv2.morphologyEx(lv, cv2.MORPH_OPEN, np.ones((ksize, ksize), np.uint8))

    n, labels, stats, _ = cv2.connectedComponentsWithStats(lv, 8)
    if n <= 1:  # Nothing found -> return original LV
        return (pred == 3)

    areas = stats[1:, cv2.CC_STAT_AREA]
    keep_ids = [i for i in np.argsort(-areas) if areas[i] >= min_area][:keep_k]
    keep = np.isin(labels, np.array(keep_ids) + 1)

    # Fallback: if everything got removed, keep original LV
    return keep if keep.any() else (pred == 3)

# Apply refiner
lv_ref = refine_lv_safe(pred_raw, keep_k=2, min_area=5, morph='close', ksize=3)
pred_ref = pred_raw.copy()
pred_ref[pred_raw == 3] = 0
pred_ref[lv_ref] = 3

# --- Metrics ---
def iou_dice(gt, pr, cls):
    g = (gt == cls)
    p = (pr == cls)
    inter = (g & p).sum()
    union = (g | p).sum()
    dice = (2 * inter) / (g.sum() + p.sum()) if (g.sum() + p.sum()) else 0.0
    iou = inter / union if union else 0.0
    return iou, dice

# Compute metrics for LV
iou_raw, dice_raw = iou_dice(gt, pred_raw, 3)
iou_ref, dice_ref = iou_dice(gt, pred_ref, 3)
print(f"LV raw   -> IoU {iou_raw:.3f}, Dice {dice_raw:.3f}")
print(f"LV refined -> IoU {iou_ref:.3f}, Dice {dice_ref:.3f}")

# --- Visualization ---
img_disp = (x[0, 0].cpu().numpy() * 0.5 + 0.5) * 255.0
img_disp = np.clip(img_disp, 0, 255).astype(np.uint8)
pred_int = pred_ref.astype(np.int64)
gt_int = gt.astype(np.int64)
mismatch = (pred_int != gt_int).astype(np.uint8) * 255
frac = float((pred_int != gt_int).mean())

plt.figure(figsize=(14, 3.8))
plt.subplot(1, 4, 1)
plt.imshow(img_disp, cmap='gray')
plt.title("Ultrasound")
plt.axis('off')
plt.subplot(1, 4, 2)
plt.imshow(index_to_rgb(gt_int))
plt.title("Ground Truth")
plt.axis('off')
plt.subplot(1, 4, 3)
plt.imshow(index_to_rgb(pred_int))
plt.title("Prediction (Refined)")
plt.axis('off')
plt.subplot(1, 4, 4)
plt.imshow(mismatch, cmap='gray')
plt.title(f"Mismatch\nFrac: {frac:.3f}")
plt.axis('off')
plt.tight_layout()
plt.savefig("test_visualization_refined.png")
plt.show()

import torch
import numpy as np
import cv2
import matplotlib.pyplot as plt

# Assumes:
# - model is AttentionUNet and loaded
# - loader_val is your validation DataLoader
# - device is your GPU/CPU
# - postprocessing function refine_lv_safe or refine_lv_safe_with_probs exists

def visualize_lv_probabilities(model, loader, device, max_images=5):
    model.eval()
    count = 0

    for imgs, masks, names in loader:
        imgs = imgs.to(device).float()
        masks = masks.cpu().numpy()  # (B,H,W)

        with torch.no_grad():
            logits = model(imgs)
            probs = torch.softmax(logits, dim=1).cpu().numpy()  # (B,C,H,W)
            preds_raw = np.argmax(probs, axis=1)  # (B,H,W)

        for i in range(imgs.shape[0]):
            gt = masks[i]
            pred = preds_raw[i]
            prob_lv = probs[i, 3]  # LV probability

            # Apply post-processing to LV
            lv_ref = refine_lv_safe(pred, keep_k=2, min_area=5)
            pred_ref = pred.copy()
            pred_ref[pred==3] = 0
            pred_ref[lv_ref] = 3

            # Compute LV metrics
            lv_gt = (gt==3)
            lv_pred = (pred_ref==3)
            inter = np.logical_and(lv_gt, lv_pred).sum()
            union = np.logical_or(lv_gt, lv_pred).sum()
            iou = inter / max(union, 1) if union else 0
            dice = (2*inter) / max(lv_gt.sum() + lv_pred.sum(), 1)

            # Overlay visualization
            img_disp = (imgs[i,0].cpu().numpy()*0.5 + 0.5)*255
            img_disp = np.clip(img_disp,0,255).astype(np.uint8)

            pred_edge = cv2.Canny(lv_pred.astype(np.uint8)*255, 10, 20)
            gt_edge   = cv2.Canny(lv_gt.astype(np.uint8)*255, 10, 20)
            overlay = cv2.cvtColor(img_disp, cv2.COLOR_GRAY2RGB)
            overlay[pred_edge>0] = [0,0,255]  # blue
            overlay[gt_edge>0] = [0,255,0]    # green

            # Probability heatmap
            plt.figure(figsize=(12,4))
            plt.subplot(1,3,1)
            plt.imshow(img_disp, cmap='gray')
            plt.title("Ultrasound")
            plt.axis('off')

            plt.subplot(1,3,2)
            plt.imshow(prob_lv, cmap='hot')
            plt.title("LV Probability Heatmap")
            plt.colorbar()
            plt.axis('off')

            plt.subplot(1,3,3)
            plt.imshow(overlay)
            plt.title(f"Refined LV Overlay\nIoU={iou:.3f}, Dice={dice:.3f}")
            plt.axis('off')

            plt.tight_layout()
            plt.show()

            count += 1
            if count >= max_images:
                return

# Call it
visualize_lv_probabilities(model, loader_val, device, max_images=5)

import os

p = "/content/drive/MyDrive/thesis/thesisdataset/Trans-thalamic/Trans-thalamic/Trans-thalamic"
MASK_DIR = "/content/drive/MyDrive/thesis/thesisdataset/Trans-thalamic/Trans-thalamic/Segmentation/SegmentationClass"

try:
    os.listdir(p); print("ACCESSIBLE")
except FileNotFoundError:
    print("NOT FOUND")
except PermissionError:
    print("NO PERMISSION")

import os, glob, cv2, numpy as np, torch, albumentations as A
from albumentations.pytorch import ToTensorV2

import os, glob

IMG_DIR  = "/content/drive/MyDrive/thesis/thesisdataset/Trans-thalamic/Trans-thalamic/Trans-thalamic"
MASK_DIR = "/content/drive/MyDrive/thesis/thesisdataset/Trans-thalamic/Trans-thalamic/Segmentation/SegmentationClass"

def find_pairs(img_root, mask_root,
               img_exts=(".png",".jpg",".jpeg"),
               mask_exts=(".png",".jpg",".jpeg")):
    # collect images (recursive)
    imgs = []
    for e in img_exts:
        imgs += glob.glob(os.path.join(img_root, "**", f"*{e}"), recursive=True)

    # collect masks (map by basename)
    mask_map = {}
    for e in mask_exts:
        for mp in glob.glob(os.path.join(mask_root, "**", f"*{e}"), recursive=True):
            base = os.path.splitext(os.path.basename(mp))[0]
            mask_map[base] = mp

    pairs = []
    for ip in sorted(imgs):
        base = os.path.splitext(os.path.basename(ip))[0]
        candidates = [
            base,                        # same name
            base + "_mask",              # foo -> foo_mask
            base + "_seg",
            base + "_label",
        ]
        if base.endswith("_image"):      # foo_image -> foo / foo_mask
            b2 = base[:-6]
            candidates += [b2, b2 + "_mask", b2 + "_seg", b2 + "_label"]

        for c in candidates:
            if c in mask_map:
                pairs.append((ip, mask_map[c]))
                break

    print(f"Paired: {len(pairs)}  | imgs scanned: {len(imgs)}  | masks indexed: {len(mask_map)}")
    if pairs[:3]:
        print("Sample pairs:\n - " + "\n - ".join([pairs[i][0] + "  ↔  " + pairs[i][1] for i in range(min(3,len(pairs)))]))
    else:
        # help debug: show some filenames we actually saw
        print("Sample imgs:", [os.path.basename(p) for p in imgs[:5]])
        print("Sample masks:", [os.path.basename(p) for p in list(mask_map.values())[:5]])
    return pairs

pairs = find_pairs(IMG_DIR, MASK_DIR)


# prep
BGR2IDX = {(0,0,0):0,(0,0,255):1,(0,255,0):2,(255,0,0):3}
def color_mask_to_index(m):
    if m.ndim==2: return np.clip(m.astype(np.int64),0,3)
    out = np.zeros(m.shape[:2], np.int64)
    for bgr, idx in BGR2IDX.items():
        out[np.all(m==np.array(bgr,np.uint8),axis=2)] = idx
    return out

ckpt  = torch.load(best_path, map_location=device)
model = UNet(in_channels=ckpt['cfg'].get('in_channels',1),
             n_classes=ckpt['cfg'].get('n_classes',4)).to(device).eval()
model.load_state_dict(ckpt['model'])

tf = A.Compose([
    A.LongestMaxSize(max_size=ckpt['cfg'].get('img_size',512)),
    A.PadIfNeeded(ckpt['cfg'].get('img_size',512), ckpt['cfg'].get('img_size',512),
                  border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),
    A.Normalize(mean=(0.5,), std=(0.5,)),
    ToTensorV2()
])

tp=fp=fn=0
for ip, mp in pairs:
    img = cv2.imread(ip, cv2.IMREAD_GRAYSCALE)
    gt  = color_mask_to_index(cv2.imread(mp, cv2.IMREAD_UNCHANGED))
    aug = tf(image=img[...,None], mask=gt); x=aug['image'].unsqueeze(0).to(device).float(); g=aug['mask'].numpy()
    with torch.no_grad(): pred = torch.argmax(model(x),1)[0].cpu().numpy()
    p = (pred==3); t = (g==3)
    tp += np.logical_and(p,t).sum()
    fp += np.logical_and(p,~t).sum()
    fn += np.logical_and(~p,t).sum()

lv_iou  = tp / max(tp+fp+fn, 1)
lv_dice = (2*tp) / max((2*tp + fp + fn), 1)
prec    = tp / max(tp+fp, 1)
rec     = tp / max(tp+fn, 1)
print(f"LV across {len(pairs)} images -> IoU {lv_iou:.3f}, Dice {lv_dice:.3f}, Precision {prec:.3f}, Recall {rec:.3f}")

import numpy as np, torch, cv2

n_classes = 4  # [bg, parenchyma, CSP, LV]
cm = np.zeros((n_classes, n_classes), dtype=np.int64)

with torch.no_grad():
    for ip, mp in pairs:  # uses your 'pairs' from the previous cell
        img = cv2.imread(ip, cv2.IMREAD_GRAYSCALE)
        gt  = color_mask_to_index(cv2.imread(mp, cv2.IMREAD_UNCHANGED))
        aug = tf(image=img[...,None], mask=gt)
        x   = aug['image'].unsqueeze(0).to(device).float()
        g   = aug['mask'].numpy()
        p   = torch.argmax(model(x), 1)[0].cpu().numpy()

        k = (g >= 0) & (g < n_classes)
        cm += np.bincount(g[k]*n_classes + p[k], minlength=n_classes**2).reshape(n_classes, n_classes)

tp = np.diag(cm); fp = cm.sum(0) - tp; fn = cm.sum(1) - tp
iou  = tp / np.maximum(tp + fp + fn, 1)
dice = (2*tp) / np.maximum(2*tp + fp + fn, 1)

print("Per-class IoU  [bg, par, CSP, LV]:", np.round(iou, 3))
print("Per-class Dice [bg, par, CSP, LV]:", np.round(dice, 3))
print("mIoU:", float(iou.mean()))
# Optional: LV-only quick view
c=3; print(f"LV IoU {iou[c]:.3f}, Dice {dice[c]:.3f}")

import numpy as np

# assuming you still have cm, iou, dice from the eval cell
tp = np.diag(cm); fp = cm.sum(0) - tp; fn = cm.sum(1) - tp
iou  = tp / np.maximum(tp + fp + fn, 1)
dice = (2*tp) / np.maximum(2*tp + fp + fn, 1)

miou_all   = float(iou.mean())
miou_no_bg = float(iou[1:].mean())       # exclude background
miou_small = float(iou[2:].mean())       # CSP+LV only
fw_iou     = float((cm.sum(1)/cm.sum() * iou).sum())  # Frequency-weighted IoU

print("IoU per class [bg, par, CSP, LV]:", np.round(iou,3))
print(f"mIoU (all): {miou_all:.3f}")
print(f"mIoU (no bg): {miou_no_bg:.3f}")
print(f"mIoU (CSP+LV): {miou_small:.3f}")
print(f"FWIoU: {fw_iou:.3f}")
print(f"LV IoU {iou[3]:.3f}, Dice {dice[3]:.3f}")

import os, cv2, numpy as np, torch, pandas as pd

def iou_dice_prec_rec(g, p):
    tp = np.logical_and(g, p).sum()
    fp = np.logical_and(~g, p).sum()
    fn = np.logical_and(g, ~p).sum()
    iou  = tp / max(tp+fp+fn, 1)
    dice = (2*tp) / max(2*tp+fp+fn, 1)
    prec = tp / max(tp+fp, 1)
    rec  = tp / max(tp+fn, 1)
    return iou, dice, prec, rec, tp, fp, fn

rows = []
with torch.no_grad():
    for ip, mp in pairs:  # uses your existing 'pairs', 'tf', 'model', 'device', 'color_mask_to_index'
        img = cv2.imread(ip, cv2.IMREAD_GRAYSCALE)
        gt  = color_mask_to_index(cv2.imread(mp, cv2.IMREAD_UNCHANGED))
        aug = tf(image=img[...,None], mask=gt)
        x   = aug['image'].unsqueeze(0).to(device).float()
        g   = aug['mask'].numpy()
        pred = torch.argmax(model(x),1)[0].cpu().numpy()
        for cls,name in [(2,'CSP'),(3,'LV')]:
            gi = (g==cls); pi = (pred==cls)
            iou,dice,prec,rec,tp,fp,fn = iou_dice_prec_rec(gi, pi)
            rows.append({
                "file": os.path.basename(ip), "class": name,
                "iou": iou, "dice": dice, "precision": prec, "recall": rec,
                "gt_pixels": int(gi.sum()), "pred_pixels": int(pi.sum()),
                "tp": int(tp), "fp": int(fp), "fn": int(fn)
            })

df = pd.DataFrame(rows)
out_csv = "/content/outputs/per_image_CSP_LV_metrics.csv"
os.makedirs(os.path.dirname(out_csv), exist_ok=True)
df.to_csv(out_csv, index=False)
print("Saved:", out_csv)
print(df.groupby("class")[["iou","dice","precision","recall"]].mean())

import pandas as pd, numpy as np
df = pd.read_csv("/content/outputs/per_image_CSP_LV_metrics.csv")

# 1) How many images actually contain the class?
has_cls = df.groupby("class")["gt_pixels"].apply(lambda s: (s>0).sum())
total    = df.groupby("class")["gt_pixels"].size()
print("Positives per class:", has_cls.to_dict(), "out of", total.to_dict())

# 2) Macro (per-image) but ONLY on images where the class is present (recommended for CSP/LV)
macro_pos = df[df["gt_pixels"]>0].groupby("class")[["iou","dice","precision","recall"]].mean()
print("\nMacro (only positives):\n", macro_pos)

# 3) Micro/global (matches your earlier confusion-matrix result)
agg = df.groupby("class")[["tp","fp","fn"]].sum()
micro_iou  = agg["tp"] / (agg["tp"] + agg["fp"] + agg["fn"]).replace(0, np.nan)
micro_dice = (2*agg["tp"]) / ((2*agg["tp"] + agg["fp"] + agg["fn"]).replace(0, np.nan))
print("\nMicro/global IoU:\n", micro_iou)
print("Micro/global Dice:\n", micro_dice)

# after you build x and run the model:
with torch.no_grad():
    logits = model(x)                     # (1,C,H,W)
probs = torch.softmax(logits, dim=1)[0].cpu().numpy()   # (C,H,W)

pred_raw = probs.argmax(axis=0)          # normal argmax

p_lv = probs[3]; p_other = probs[[0,1,2]].max(axis=0)
lv_gate = (p_lv >= 0.50) & ((p_lv - p_other) >= 0.08)   # was 0.55 / 0.12

pred_g = pred.copy()
pred_g[pred==3] = 0
pred_g[lv_gate] = 3


# (optional) gentle cleanup
# pred_gated = keep_largest_component(pred_gated, cls=3, min_area=5)

def iou_dice(gt, pr, cls=3):
    g=(gt==cls); p=(pr==cls)
    inter=(g&p).sum(); uni=(g|p).sum()
    dice=(2*inter)/max(g.sum()+p.sum(),1)
    iou = inter/max(uni,1)
    return iou, dice

iou_raw,dice_raw   = iou_dice(gt, pred_raw,   3)
iou_gate,dice_gate = iou_dice(gt, pred_gated, 3)
print(f"LV raw   -> IoU {iou_raw:.3f}, Dice {dice_raw:.3f}")
print(f"LV gated -> IoU {iou_gate:.3f}, Dice {dice_gate:.3f}")

